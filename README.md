# Tigrinya Web Scraper

A full-stack web application for scraping and processing Tigrinya news articles from shabait.com, with a React frontend and FastAPI backend. Includes **LlamaIndex** ingestion into **Qdrant** and **RAG** to answer Tigrinya questions (similar to [tigrinya-agent](https://github.com/tsegaimerhawi/tigrinya-agent)).

## Features

- ğŸ•·ï¸ **Automated Scraping**: Downloads Haddas Ertra newspaper PDFs
- ğŸ“„ **Multi-page Navigation**: Handles pagination to access older articles
- ğŸ” **Smart PDF Detection**: Locates download links using image-based navigation
- ğŸ§¹ **Text Cleaning**: Removes English words, navigation elements, and noise
- ğŸŒ **Ge'ez Script Focus**: Preserves only Tigrinya characters, numbers, and punctuation
- ğŸŒ **Web Interface**: Modern React frontend for scraping, articles, and RAG Q&A
- ğŸ“Š **NLP Tools**: Word frequency, text statistics, sentence extraction, and more
- ğŸ“‹ **Copy Text**: Easy one-click copy of extracted article text
- ğŸ“¦ **LlamaIndex + Qdrant**: Store processed text as embeddings in a vector database
- ğŸ¤– **RAG**: Ask questions in Tigrinya or English; answers use the ingested news corpus
- ğŸ–¥ï¸ **Script Runner UI**: Separate dashboard (port 8765) to run Scrape â†’ Process â†’ Ingest with live output and configuration (like [tigrinya-agent](https://github.com/tsegaimerhawi/tigrinya-agent))

## Prerequisites

- **Python 3.8+** - [Download Python](https://www.python.org/downloads/)
- **Node.js 18+ and npm** - [Download Node.js](https://nodejs.org/)
- **Git** - [Download Git](https://git-scm.com/downloads)
- **Qdrant** (optional, for RAG): run with Docker: `docker run -p 6333:6333 qdrant/qdrant`
- **Google Gemini API key** (for NER, image descriptions, RAG embeddings and answers): set in `config.env` as `GEMINI_API_KEY` or `GOOGLE_API_KEY`

Verify installations:
```bash
python3 --version  # Should be 3.8 or higher
node --version     # Should be 18 or higher
npm --version
```

## Installation

### Clone the Repository

```bash
git clone https://github.com/tsegaimerhawi/tigrinya-web-scraper.git
cd tigrinya-web-scraper
```

### Install Dependencies

**Backend:**
```bash
python3 -m venv .env
source .env/bin/activate   # On Windows: .env\Scripts\activate
pip install -r backend/requirements.txt
playwright install chromium
```

**Frontend:**
```bash
cd frontend
npm install
cd ..
```

### Configuration

- Copy `config.env.example` to `config.env` and set **GEMINI_API_KEY** (or **GOOGLE_API_KEY**) for Gemini.
- Optional: set **QDRANT_HOST**, **QDRANT_PORT**, **QDRANT_COLLECTION** if not using defaults (localhost:6333, collection `tigrinya_llamaindex`).
- **Frontend API URL**: Edit `frontend/.env` if the backend runs on a different port (default: `http://localhost:8000`).
- **Data directory**: Set `TIGRINYA_DATA_DIR` to change where PDFs and `raw_data.json` are stored (default: project root).

## Quick Start

### Option 1: Startup Scripts

**Terminal 1 â€“ Backend:**
```bash
./start-backend.sh
```

**Terminal 2 â€“ Frontend:**
```bash
./start-frontend.sh
```

Then open **http://localhost:5173** for the main app and use the **Scrape**, **Articles**, and **Ask (RAG)** tabs.

### Option 2: Script Runner UI (Scrape â†’ Process â†’ Ingest)

Run the standalone Script Runner (similar to [tigrinya-agent](https://github.com/tsegaimerhawi/tigrinya-agent)):

```bash
source .env/bin/activate
pip install -r backend/requirements.txt
python script_runner.py
```

Open **http://localhost:8765**. You can:

- **Configuration** â€“ Set scraper limit, Qdrant host/port, collection name, batch sizes (saved to `runner_config.json`).
- **Scraper** â€“ Download Haddas Ertra PDFs (uses `--limit` from config).
- **PDF Processor** â€“ Extract and clean text from PDFs; writes `raw_data.json`.
- **Llama Ingest** â€“ Ingest `raw_data.json` into Qdrant with LlamaIndex (Gemini embeddings).
- **Check Qdrant** â€“ Verify Qdrant is running and list collections.
- **Validate Results** â€“ Check `pdf_metadata.json` and `raw_data.json` counts.

Output streams in real time. Use this UI to scrape, preprocess, extract, and store news data without running the React app.

### Option 3: Manual Backend + Frontend

**Terminal 1 â€“ Backend:**
```bash
source .env/bin/activate
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Terminal 2 â€“ Frontend:**
```bash
cd frontend
npm run dev
```

### RAG (Ask Tigrinya Questions)

1. **Ingest data** (once): Use the Script Runner UI (Llama Ingest) or call `POST /ingest` (or run `python llama_ingest.py` from project root). Ensure Qdrant is running and `raw_data.json` exists (run Scraper + PDF Processor first).
2. **Ask questions**: In the main app, open the **Ask (RAG)** tab and type a question in Tigrinya or English; answers are generated from the ingested corpus using Gemini.

You can also call the API directly: `POST /rag/ask` with body `{"question": "áŠ¤áˆ­á‰µáˆ« áŠ¥áŠ•á‰³á‹­ áŠ¥á‹«?", "k": 5}`.

## Output Files

- `pdf_metadata.json` â€“ Metadata about downloaded PDFs (URLs, titles, dates, file paths)
- `raw_data.json` â€“ Processed text data with cleaned Ge'ez script content (used by Llama Ingest)
- `pdfs/` â€“ Directory containing downloaded PDF files
- `runner_config.json` â€“ Script Runner configuration (scraper limit, Qdrant, batch sizes)

## API Endpoints

- `GET /newspapers` â€“ List available newspapers
- `POST /scrape` â€“ Start scraping articles
- `GET /scrape/status` â€“ Get scraping status
- `POST /process` â€“ Run PDF processing (extract text, NER, image descriptions)
- `GET /process/status` â€“ Get processing status
- `GET /articles` â€“ List processed articles
- `GET /articles/{index}/text` â€“ Get full text of an article
- `POST /nlp/word-frequency`, `POST /nlp/stats`, `POST /nlp/sentences`, `POST /nlp/dedupe-lines` â€“ NLP utilities
- **Ingest & RAG**
  - `POST /ingest` â€“ Run LlamaIndex ingestion (raw_data.json â†’ Qdrant)
  - `POST /rag/ask` â€“ Answer a question using RAG (body: `{"question": "...", "k": 5}`)
  - `POST /rag/search` â€“ Semantic search only (body: `{"query": "...", "k": 5}`)

See **http://localhost:8000/docs** for interactive API documentation.

## Configuration

- **Backend**: `backend/app/config.py` â€“ newspapers, data paths, Qdrant host/port/collection.
- **Frontend**: `frontend/.env` â€“ API base URL (default `http://localhost:8000`).
- **Script Runner**: Use the Configuration button in the Script Runner UI, or edit `runner_config.json`.
- **Secrets**: `config.env` â€“ `GEMINI_API_KEY` or `GOOGLE_API_KEY`; optional `QDRANT_*` overrides.

## Text Cleaning

The PDF processor (and backend `pdf_service`) clean text by:

- Removing English words and navigation elements (bullets, symbols)
- Removing page numbers, dates, URLs
- Keeping only Ge'ez script characters (U+1200â€“U+137F), numbers, and standard punctuation
- Filtering out lines with too many special characters

## License

This project is for educational and research purposes. Please respect website terms of service and copyright laws when using the scraped content.

## Author

Tsegai Merhawi â€“ Tigrinya newspaper digitization project
